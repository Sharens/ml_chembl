{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from polars import col\n",
    "from libs.misc import *\n",
    "from libs.misc import load_and_clean_data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c193fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = 'libs/datasets/chembl_selected_ds.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bff602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_parquet(RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53453a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fetch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limited = df.collect().slice(0, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dae74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limited.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36415d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unit = impute_units(df_limited, value_col=\"standard_value\", units_col=\"standard_units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f460dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calc = compute_pIC50(df_unit)\n",
    "df_calc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f26c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zaokrąglanie Pica\n",
    "df_calc = df_calc.with_columns(\n",
    "    pl.col(\"pIC50\")\n",
    "    .round(2)                 # Najpierw naprawiamy błąd precyzji float\n",
    "    .cast(pl.Decimal(None, 2)) # Potem blokujemy na 2 miejscach po przecinku\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False bo źle sie zaokrągla\n",
    "df_calc[\"pchembl_value\"].cast(pl.Decimal(scale=2)).equals(df_calc[\"pIC50\"].cast(pl.Decimal(scale=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d46154",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "N_VALUE = 100000\n",
    "EPOCH_NUM = 50\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce127273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PRZYGOTOWANIE DANYCH ---\n",
    "RAW_PATH = 'libs/datasets/chembl_selected_ds.parquet' # Ścieżka do Twojego pliku\n",
    "df = load_and_clean_data(RAW_PATH, N_VALUE, RANDOM_SEED)\n",
    "\n",
    "smiles = df[\"canonical_smiles\"].to_list()\n",
    "targets = df[\"pIC50\"].to_numpy()\n",
    "\n",
    "# Dzielimy dane (najlepiej używać Scaffold Split w chemii, tu dla uproszczenia Random Split)\n",
    "X_smiles_train, X_smiles_test, y_train, y_test = train_test_split(smiles, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "\n",
    "# === ŚCIEŻKA 1: SZKOLENIE MLP (na Fingerprintach) ===\n",
    "print(\"\\n=== Rozpoczynanie ścieżki MLP ===\")\n",
    "\n",
    "# Generowanie features\n",
    "X_fp_train, idx_train = generate_fingerprints(X_smiles_train)\n",
    "y_train_mlp = y_train[idx_train]\n",
    "\n",
    "X_fp_test, idx_test = generate_fingerprints(X_smiles_test)\n",
    "y_test_mlp = y_test[idx_test]\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset_mlp = MoleculeDatasetMLP(X_fp_train, y_train_mlp)\n",
    "test_dataset_mlp = MoleculeDatasetMLP(X_fp_test, y_test_mlp)\n",
    "train_loader_mlp = DataLoader(train_dataset_mlp, batch_size=64, shuffle=True)\n",
    "test_loader_mlp = DataLoader(test_dataset_mlp, batch_size=64)\n",
    "\n",
    "# Inicjalizacja MLP\n",
    "mlp_model = BioActivityMLP().to(device)\n",
    "optimizer_mlp = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Trening MLP\n",
    "print(\"Trenowanie MLP...\")\n",
    "mlp_model.train()\n",
    "for epoch in range(EPOCH_NUM): # Zwiększ liczbę epok dla lepszych wyników\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader_mlp:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer_mlp.zero_grad()\n",
    "        output = mlp_model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_mlp.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader_mlp):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ŚCIEŻKA 2: SZKOLENIE GNN (na Grafach) ===\n",
    "print(\"\\n=== Rozpoczynanie ścieżki GNN ===\")\n",
    "\n",
    "# Konwersja do obiektów Data\n",
    "train_graphs = []\n",
    "for s, y in zip(X_smiles_train, y_train):\n",
    "    g = smile_to_graph(s, y)\n",
    "    if g: train_graphs.append(g)\n",
    "\n",
    "test_graphs = []\n",
    "for s, y in zip(X_smiles_test, y_test):\n",
    "    g = smile_to_graph(s, y)\n",
    "    if g: test_graphs.append(g)\n",
    "\n",
    "# GeoDataLoader obsługuje batchowanie grafów o różnych rozmiarach\n",
    "train_loader_gnn = GeoDataLoader(train_graphs, batch_size=64, shuffle=True)\n",
    "test_loader_gnn = GeoDataLoader(test_graphs, batch_size=64)\n",
    "\n",
    "# Inicjalizacja GNN\n",
    "gnn_model = BioActivityGNN(node_features_dim=13).to(device)\n",
    "optimizer_gnn = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Trening GNN\n",
    "print(\"Trenowanie GNN...\")\n",
    "gnn_model.train()\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader_gnn:\n",
    "        batch = batch.to(device)\n",
    "        optimizer_gnn.zero_grad()\n",
    "        output = gnn_model(batch)\n",
    "        loss = criterion(output, batch.y.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer_gnn.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader_gnn):.4f}\")\n",
    "\n",
    "# === EWALUACJA (Przykład dla GNN) ===\n",
    "gnn_model.eval()\n",
    "preds = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_gnn:\n",
    "        batch = batch.to(device)\n",
    "        out = gnn_model(batch)\n",
    "        preds.extend(out.cpu().numpy().flatten())\n",
    "        actuals.extend(batch.y.cpu().numpy().flatten())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "r2 = r2_score(actuals, preds)\n",
    "\n",
    "print(f\"\\nWyniki GNN na zbiorze testowym:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
